incomplete dot product dynamic computation scale neural network inference machine learningapproximate computingiotdeep learningconvolutional neural network propose use incomplete dot product idp dynamically adjust number input channel use each layer convolutional neural network during feedforward inference idp add monotonically nonincreasing coefficient refer profile channel during training profile order contribution each channel nonincreasing order inference time number channel use dynamically adjust trade off accuracy lower power consumption reduce latency by select only begin subset channel this approach allow single network dynamically scale over computation range oppose train deploy multiple network support different level computation scale additionally extend notion multiple profile each optimize some specific range computation scale present experiment computation accuracy tradeoff idp popular image classification model datasets demonstrate that mnist idp reduce computation significantly eg by without significantly compromise accuracy argue that idp provide convenient effective mean device low computation cost dynamically reflect current computation budget system example with idp use only first channel achieve accuracy dataset compare standard network which achieve only accuracy when use reduce channel set