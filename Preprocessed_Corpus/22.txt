cyclic contrastive divergence learn algorithm highorder rbms highorder rbms cyclic contrastive divergence learn gradient approximation convergence upper bound restrict boltzmann machine rbm special case general boltzmann machine typical probabilistic graphical model attract much attention recent year due it powerful ability extract feature represent distribution underlie train data most commonly use algorithm learn rbms call contrastive divergence cd propose by hinton which start markov chain data point run chain only few iteration get low variance estimator however when refer highorder rbm since there interaction among it visible layer gradient approximation via cd learn usually become far from loglikelihood gradient even may cause cd learn fall into infinite loop with high reconstruction error this paper new algorithm name cyclic contrastive divergence ccd introduce learn highorder rbms unlike standard cd algorithm ccd update parameter accord each visible layer turn by borrow idea cyclic block coordinate descent method evaluate performance propose ccd algorithm regard highorder rbms learn both algorithms ccd standard cd theoretically analyze include convergence estimate upper bound both bias comparison from which superiority ccd learn reveal experiment handwritten digit classification task mnist dataset perform experimental result show that ccd more applicable consistently outperform standard cd both convergent speed performance