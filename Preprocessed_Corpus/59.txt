improve knn rule small training set error analysis training prediction algorithm data model computer science educational institution electronic mail traditional knn classification rule predict label base most common label k near neighbor plurality rule it know that plurality rule optimal when number examples tend infinity this paper show that plurality rule suboptimal when number label large number example small propose simple knn rule that take into account label neighbor rather than just most common label present number experiment both synthetic datasets realworld datasets include mnist svhn show that our new rule achieve low error rate compare majority rule many case