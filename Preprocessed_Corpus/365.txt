anytime exploitation straggler synchronous stochastic gradient descent distribute sgdstochastic gradient descentparallelized sgdstraggler this paper propose an approach parallelize synchronous stochastic gradient descent sgd that term anytimegradients anytimegradients design exploit work complete by slow compute node or straggler many approach work complete by these node while only partial discard completely maintain synchronization our approach each computational epoch fix duration end each epoch worker send updated parameter vector master mode combination master weight each update by amount work do anytimegradients scheme robust both persistent nonpersistent straggler require prior knowledge about processor ability show that scheme effectively exploit straggler outperforms exist method