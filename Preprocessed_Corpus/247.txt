assess threat adversarial example deep neural network neural networkssecurityagriculturetrainingmachine learningmimicscameras deep neural network face potential security threat from adversarial example input that look normal but cause an incorrect classification by deep neural network example propose threat could result handwritten digit scan check be incorrectly classify but look normal when human see them this research assess extent which adversarial example pose security threat when one considers normal image acquisition process this process mimic by simulate transformation that normally occur acquire image real world application such use scanner acquire digit check amount or use camera an autonomous car these small transformation negate effect carefully craft perturbation adversarial example result correct classification by deep neural network thus just acquire image decrease potential impact propose security threat also show that already widely use process average over multiple crop neutralize most adversarial example normal preprocessing such text binarization almost completely neutralizes adversarial example this first paper show that text driven classification adversarial examples an academic curiosity not security threat