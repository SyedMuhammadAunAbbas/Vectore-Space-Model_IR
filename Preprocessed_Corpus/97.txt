semisupervised cluster approach semantic slot label natural language processing semantic slot label semisupervised learn interactive system work train semantic slot labellers use natural language processing application typically either rely large amount label input data or assume entirely unlabelled input former technique tend costly apply while latter often not accurate it supervised counterpart here present semisupervised learn approach that automatically label semantic slot set train data aim strike balance between dependence label data prediction accuracy essence our algorithm cluster clause base similarity function that combine lexical semantic information present experiment that compare different similarity function both our semisupervised set fully unsupervised baseline while semisupervised learn expectedly outperforms unsupervised learn our result show that this effect observe base very few training data instance that increase size training data do not lead good performance that lexical semantic information contribute differently different domain so that cluster base both type information offer best generalisation