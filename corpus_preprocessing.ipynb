{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decc606a",
   "metadata": {},
   "source": [
    "# **Corpus Preprocessing File:**\n",
    "### *The following file contains the preprocessing and exploratory analysis performed on th corpus at hand. Our goal is to get familiar with the corpus, perform preprocessing steps on each document, including case foldings, stop-word removal and lemmatization, and prepare the documents for inverted index formation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59d79ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and modules used\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import chardet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77520371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\11th Generation\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\11th\n",
      "[nltk_data]     Generation\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c681d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'Abstracts'\n",
    "STOPWORD_LIST = 'Stopword-List.txt'\n",
    "PREPRCESS_RSLT = 'Preprocessed_Corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7ca81fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(PREPRCESS_RSLT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28d31014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords list provided---> {'we', 'for', 'be', 'her', 'up', 'do', 'as', 'the', 'all', 'once', 'had', 'and', 'to', 'is', 'at', 'am', 'has', 'in', 'are', 'on', 'no', 'can', 'of', 'have', 'his', 'a'}\n"
     ]
    }
   ],
   "source": [
    "# Getting Stopwords into a set for effecient stop word removal.\n",
    "\n",
    "with open(STOPWORD_LIST, 'r', encoding= 'utf-8') as f:\n",
    "    \n",
    "    stopwords= set(f.read().split())\n",
    "    \n",
    "print(f'Stopwords list provided---> {stopwords}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0864878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originally --> 112.txt : Windows-1252\n",
      "Originally --> 116.txt : ISO-8859-1\n",
      "Originally --> 121.txt : ISO-8859-1\n",
      "Originally --> 165.txt : ISO-8859-1\n",
      "Originally --> 229.txt : ISO-8859-1\n",
      "Originally --> 256.txt : ISO-8859-1\n",
      "Originally --> 275.txt : ISO-8859-1\n",
      "Originally --> 287.txt : ISO-8859-1\n",
      "Originally --> 307.txt : ISO-8859-1\n",
      "Originally --> 319.txt : Windows-1252\n",
      "Originally --> 336.txt : Windows-1252\n",
      "Originally --> 365.txt : Windows-1252\n",
      "Originally --> 371.txt : Windows-1252\n",
      "Originally --> 379.txt : ISO-8859-1\n",
      "Originally --> 392.txt : ISO-8859-1\n",
      "Originally --> 420.txt : Windows-1252\n",
      "Originally --> 423.txt : ISO-8859-1\n",
      "Originally --> 424.txt : ISO-8859-1\n",
      "Originally --> 434.txt : ISO-8859-1\n",
      "Originally --> 81.txt : ISO-8859-1\n",
      "Originally --> 85.txt : ISO-8859-1\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Checking if there are any documents present in the corpus \n",
    "    which might have a different encoding then utf-8\n",
    "    Then normalizing all files to utf-8 encoding to ensure avoiding problems in the future\n",
    "\"\"\"\n",
    "\n",
    "def Detect_Encoding(doc_path):\n",
    "    \n",
    "    with open(doc_path, 'rb') as f:\n",
    "        \n",
    "        data= f.read()\n",
    "        \n",
    "        result= chardet.detect(data)\n",
    "        \n",
    "        return result['encoding']\n",
    "\n",
    "def encode_to_utf_8(file_path, original_encoding):\n",
    "    \n",
    "    with open(file_path, 'r', encoding= original_encoding, errors= 'replace') as f:\n",
    "\n",
    "        data= f.read()\n",
    "        \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "\n",
    "        f.write(data)\n",
    "\n",
    "for document in os.listdir(CORPUS):\n",
    "    \n",
    "    doc_path= os.path.join(CORPUS, document)\n",
    "    \n",
    "    encoding = Detect_Encoding(doc_path)\n",
    "    \n",
    "    if encoding not in ('utf-8', 'ascii'):\n",
    "        \n",
    "        print(f'Originally --> {document} : {encoding}')\n",
    "        \n",
    "        encode_to_utf_8(doc_path, encoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb490af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing Text including:\n",
    "    1) Case Folding\n",
    "    2) Handling Punctuations\n",
    "    3) Stop Word Removal\n",
    "    4) Tokenization and Lemmatization\n",
    "\"\"\"\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def text_preprocesser(text):\n",
    "    \n",
    "    text= text.lower()\n",
    "    \n",
    "    text= text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    words= word_tokenize(text)\n",
    "    \n",
    "    words= [word for word in words if word.isalpha() and word not in stopwords]\n",
    "    \n",
    "    pos_tgs= nltk.pos_tag(words)\n",
    "    \n",
    "    lemmas= [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tgs]\n",
    "    \n",
    "    return \" \".join(lemmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "297fa5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this test out textpreprocesser step function preprocessing text our corpus run runner run stop recommend\n"
     ]
    }
   ],
   "source": [
    "# Testing word preprocessor\n",
    "\n",
    "test= \"Hello, this is testing OUT the text_preprocesser steps function for preprocessing text in our corpus ran runner run stopped recommended..\"\n",
    "\n",
    "rslt= text_preprocesser(test)\n",
    "\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f71d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All files processed successfully!\n"
     ]
    }
   ],
   "source": [
    "for document in os.listdir(CORPUS):\n",
    "    path = os.path.join(CORPUS, document)\n",
    "    try:\n",
    "        \n",
    "        with open(path, 'r', encoding= 'utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        processed_text = text_preprocesser(text)\n",
    "\n",
    "        new_path = os.path.join(PREPRCESS_RSLT, document)\n",
    "        with open(new_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(processed_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {document} due to {e}\")\n",
    "\n",
    "print(\"\\n All files processed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead106c",
   "metadata": {},
   "source": [
    "# We have succesfully preprocessed the corpus and have made the choice to save the preprocessed version of each document in the **Preprocessed_Corpus** directory... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
